Forecast Application Development Plan
Data Ingestion and Preprocessing
	•	CSV Input: Use Streamlit’s file uploader to accept a CSV file with the expected columns: Week Start, Week End, Item Id, Inventory, Product Type, GMV, Page Views, EBC Page Views, Units Sold, Ad Spend, Ad Sales. The app will parse the uploaded file into a Pandas DataFrame . Provide user guidance (e.g., a brief description or an example file format) so they know the required column names and formats. After upload, display a sample of the data (first few rows) on the UI to confirm successful ingestion .
	•	Column Validation: Verify that all required columns are present and correctly named. If any columns are missing or mislabeled, show a clear error message to the user (e.g., “Column X is missing from the uploaded file”). Ensure the date columns (Week Start and Week End) are parsed as datetime objects and in chronological order. If needed, use Week Start as the primary date index for forecasting (assuming each row represents a week-long period starting on that date).
	•	Item Selection: If the CSV contains multiple Item Id values (multiple products), allow the user to select which item to forecast. For example, use a dropdown (Streamlit selectbox) populated with unique Item Ids from the data. Once an item is selected, filter the DataFrame to only that Item’s data for modeling. This ensures the forecasts are specific to a single product at a time, simplifying the modeling process.
	•	Handling Missing Data: Check for missing or null values in the key columns (especially in the date or Units Sold columns). If Units Sold has missing entries, decide on a strategy: fill with 0 (if a week had no sales) or interpolate if appropriate. Ensure that the time series has continuous weekly entries; if any weeks are missing in the sequence, consider inserting them with Units Sold = 0 (or carrying forward other values as needed) so that models like ARIMA and LSTM can train on a complete sequence. Also handle outliers (e.g., extremely large sales values) by either capping or flagging them, since they could skew some models.
	•	Feature Engineering: Create any additional features needed for certain models. For instance, generate lag features (previous weeks’ sales) and moving averages for use in XGBoost or LSTM models . Lag features involve using past values of Units Sold as predictors for future values . You can also derive time-based features like week number, month, or year if the data spans multiple years (to capture seasonal patterns). However, keep the feature set simple for initial development – the focus is on forecasting Units Sold, so the primary input to models will be the historical Units Sold time series along with any exogenous features that might improve accuracy (such as Page Views or Ad Spend if you choose to include them as regressors in certain models).
	•	Train/Test Split: For model evaluation purposes, split the historical data into a training set and a validation/test set. A common approach is to use the most recent few weeks as a test set (e.g., the last 4 or 8 weeks of available data) and train on the earlier weeks. This split will allow computation of forecast accuracy metrics by comparing model predictions to the actual values in the held-out test period. Make sure the split is time-based (chronological) rather than random, to respect the time series nature. Keep the test set aside and do not use it for training any model, to ensure the metrics reflect true predictive performance.
Forecasting Models Implementation
Implement four forecasting models – Prophet, ARIMA, LSTM, and XGBoost – each offering a different approach to time series prediction. The application will train each model on the provided historical data (training set) and generate forecasts for the specified number of future weeks. Key considerations for each model are outlined below:
Prophet (Facebook Prophet)
	•	Model Choice: Prophet is a robust time series forecasting library well-suited for data with seasonality and trends. It automatically detects yearly, weekly, and daily seasonal patterns and trend changes, making it convenient for business time series data . Prophet treats forecasting as a curve-fitting problem with components (trend, seasonality, holidays) and typically requires a DataFrame with columns ds (date) and y (value).
	•	Implementation: Convert the data for the selected Item Id into Prophet’s expected format: for example, create a DataFrame with ds = Week Start date and y = Units Sold. Include additional regressors if desired (Prophet allows adding extra features like promotions or page views, but initially you may skip this for simplicity). Instantiate a Prophet model object and fit it on the historical data . Since the data is weekly, Prophet will automatically handle weekly seasonality; you can explicitly enable yearly seasonality if the data spans multiple years.
	•	Forecast Generation: After fitting, use make_future_dataframe to extend the timeline by the user-specified number of weeks into the future . Then call model.predict(future_dates) to generate the forecast. This will produce a DataFrame containing forecasted yhat values for each future week, along with confidence intervals (yhat_lower, yhat_upper). Filter this forecast to only the future dates beyond the latest actual date .
	•	Post-processing: The forecast results can be merged or compared with the actual data for plotting. Prophet also provides components plots (trend and seasonality breakdown) which can be displayed for additional insight if time allows. In the context of this app, Prophet’s output will be one forecast line for Units Sold over time.
ARIMA (AutoRegressive Integrated Moving Average)
	•	Model Choice: ARIMA is a classical statistical model for time series forecasting that uses autocorrelation structure in the data. It’s widely used for forecasting sales and inventory because it models future values as a linear combination of past values and past errors . ARIMA has three parameters: p (autoregression order), d (differencing order to make data stationary), q (moving average order). Seasonal ARIMA (SARIMA) adds seasonal terms, but if the data is weekly and spans multiple years, a seasonal component (e.g., annual seasonality) could be incorporated as well.
	•	Implementation: Before fitting ARIMA, ensure the time series is stationary (constant mean and variance over time). If the weekly Units Sold data shows a clear trend or seasonality, apply differencing (d=1 or seasonal differencing) to remove trends . You can use libraries like pmdarima (AutoARIMA) to automatically select the best p, d, q parameters based on AIC/BIC, which simplifies development. For example, call pmdarima.auto_arima on the training series (with seasonality set to True and m=52 if yearly seasonality is expected for weekly data) to find an optimal ARIMA model. Alternatively, use statsmodels’ ARIMA/ARIMAX implementation if manual control is needed.
	•	Exogenous Inputs: Optionally include other variables (like Page Views or Ad Spend) as exogenous regressors in an ARIMAX model if they are believed to influence sales. This would involve providing those series (aligned to the same dates) to the model. However, including exogenous variables increases complexity; the initial version can focus on univariate ARIMA using only past Units Sold.
	•	Forecast Generation: Once the ARIMA model is fitted on the training data, generate forecasts for the next N weeks (the user-defined horizon). Most ARIMA implementations provide a forecast or predict method where you specify the number of periods into the future. This returns forecasted values and can often provide confidence intervals as well. Collect the forecasted Units Sold for the future weeks. If a test set was set aside, also produce forecasts for the test period (the weeks of the test set) so that accuracy metrics can be computed by comparing to actuals.
	•	Model Performance: Note that ARIMA can be slower on large datasets, and model fitting can fail if the data is not well-behaved (e.g., if non-stationary without proper differencing). Monitor for warnings or errors and handle them (for example, by catching exceptions or fallback to a simpler ARIMA(0,1,0) model as a baseline if auto-selection fails).
LSTM (Long Short-Term Memory Neural Network)
	•	Model Choice: LSTM is a type of recurrent neural network well-suited for sequence data like time series. LSTMs can capture long-term dependencies and non-linear relationships in the data , which may improve forecasting for complex patterns that aren’t well handled by linear models. However, they require more data and tuning, and are computationally heavier.
	•	Data Preparation: Convert the time series into supervised learning form. This means creating sequences of past weeks’ Units Sold values to predict the next week’s value. For example, for a lag window of 3, the features for a given week would be [UnitsSold_{t-3}, UnitsSold_{t-2}, UnitsSold_{t-1}] and the target is UnitsSold_t. Prepare the training input arrays X (shape: samples × timesteps × features) and target vector y. Because this is a univariate series (assuming we only use past sales), X will be of shape (samples, timesteps, 1). Scale the data (using MinMaxScaler or similar) to help the neural network training converge faster.
	•	Implementation: Build an LSTM model using TensorFlow/Keras. A simple architecture might consist of an LSTM layer with a certain number of units (neurons) followed by a Dense output layer with one neuron (to predict the next week’s sales). For example, model = Sequential([LSTM(50, activation='relu', input_shape=(timesteps, 1)), Dense(1)]) could serve as a starting point. Compile the model with a regression loss (e.g., mean squared error) and an optimizer like Adam. Train the model on the training data for a number of epochs (monitoring validation loss on the test set if doing a train/test split) until it converges or a max epoch count is reached.
	•	Forecast Generation: To forecast N weeks ahead, one approach is iterative prediction: start with the last timesteps of the known data, predict the next week, then append that prediction and use it (dropping the oldest week in the window) to predict the following week, and so on for the required horizon. This way, the model uses its own prior predictions for multi-step forecasting. Another approach is to restructure the LSTM to predict multiple steps ahead in one go (sequence-to-sequence model), but that adds complexity. For simplicity, the iterative single-step forecasting approach will be used. The result will be a list of N forecasted values for Units Sold.
	•	Considerations: LSTM training can be slow on CPU. To keep the app responsive, consider limiting the epochs or using a small network architecture initially. You might also use caching (Streamlit st.cache_data) so that if the same model training is triggered again it can reuse results. Ensure that the random initialization or any stochastic training aspect is handled (for example, fix a random seed for reproducibility). Since this is for local testing, the user should be informed if the LSTM model training will take a noticeable amount of time.
XGBoost Regressor
	•	Model Choice: XGBoost is a powerful gradient boosting tree algorithm that can be applied to time series forecasting by using a supervised learning approach. It’s known for handling non-linear relationships and providing feature importance, which can be useful in understanding the drivers of the forecast . Unlike Prophet and ARIMA which inherently work with time-indexed data, XGBoost requires feature engineering to incorporate time dependencies (since it has no built-in notion of order or lag).
	•	Feature Engineering: Prepare a training dataset with features derived from the time series. A common technique is to use lag features: include columns for the sales value in previous weeks . For example, create features like lag_1 = UnitsSold_{t-1}, lag_2 = UnitsSold_{t-2}, …, up to lag_k for some chosen lag window (say k = 3 or 4). Also consider adding other available variables as features: inventory levels, page views, ad spend, etc., at time t (or lagged as well if they may have a delayed effect on sales). Each training sample will represent a week, with features being the values from previous weeks and possibly concurrent exogenous features, and the target being Units Sold for that week.
	•	Implementation: Split the feature-engineered DataFrame into train X and y. Use the XGBoost regressor (e.g., xgboost.XGBRegressor from the XGBoost Python library) and train it on the training set. Typical hyperparameters (like number of trees, max depth, learning rate) can be tuned, but to start, use default parameters or a modest number of trees to ensure the model trains quickly. XGBoost can handle missing values in features internally , but ideally our lag features will be filled (the first few rows might have NaN lags which should be dropped).
	•	Forecast Generation: Similar to LSTM’s iterative approach, use the trained XGBoost model to predict future weeks one step at a time. For example, to predict week t+1, take the last k weeks of actual data (for lags) and current values of other features (if any) to form the feature vector, then predict. Append this prediction to the series, update the lag features, and predict week t+2, and so forth. This loop produces N forecasted points. (Alternatively, one could train the model to predict multiple steps ahead if we create features for specific future time indices, but the iterative method is straightforward.) Ensure that any exogenous feature needed for future predictions is known or can be assumed – if not, you may hold them constant or use the last known value.
	•	Insights: After training, XGBoost can provide a feature importance plot or values, which helps identify whether, say, last week’s sales or page views are the strongest predictors. While this is a nice-to-have for the app, initially you might just log or print these to console for verification. Focus on making sure the predictions make sense (e.g., no negative sales – if the model does predict negative, you might cap at zero, since sales can’t be negative).
User Interface and UX Considerations (Streamlit)
	•	Layout: Design a clean, single-page interface in Streamlit. Use a title (e.g., st.title("Weekly Sales Forecast Application")) to identify the app, and divide the app into sections using headers or subheaders (e.g., “Upload Data”, “Forecast Settings”, “Results”). Streamlit’s structure (running top to bottom on each interaction) means you will place the UI elements in logical order: first the file upload, then the parameter inputs (like horizon, item selection, model selection), then a button to trigger forecasting (or automatically trigger when inputs are set), and finally the output displays (metrics and plots).
	•	File Upload: Implement st.file_uploader for the CSV input . Provide the accepted file type (type='csv') and a clear label (e.g., “Upload weekly sales data CSV”). Once a file is uploaded, read it into a DataFrame and perform the preprocessing as described. Immediately show a message or a small table preview of the data, e.g., st.write(df.head()), so the user can verify that the file was read correctly (column names, sample values) . If the dataset is large, consider showing just a few lines or using st.dataframe(df.sample(5)). If multiple Item Ids are present, show a selector for the user to pick one (e.g., st.selectbox("Select Item ID to Forecast:", options=df['Item Id'].unique())).
	•	Forecast Parameters: Allow the user to specify the forecast horizon (number of weeks to predict). Use a number input or slider component for this. For example, st.number_input("Weeks to forecast:", min_value=1, max_value=52, value=8) which lets them choose between 1 and 52 weeks, defaulting to 8 weeks (about two months) . You might place this in a sidebar for clarity, along with any other controls, or keep it in the main section under a “Forecast Settings” subheader. If needed, also allow selection of which models to run: you could use checkboxes (e.g., st.checkbox("Use Prophet"), etc.) or a multiselect to pick one or multiple models. By default, all models could be selected to run for comparison.
	•	Action Trigger: It’s often user-friendly to have a button like “Run Forecast” which, when clicked, will execute the training and forecasting for the chosen models. This ensures the heavy computation doesn’t automatically run on every small change (which could be slow, especially with LSTM). Implement this by wrapping the model execution code inside something like if st.button("Run Forecast"): so it only runs when pressed. You can also use st.spinner("Training models...") context manager to show a spinner during computation, giving feedback that something is happening.
	•	Displaying Results: Once forecasting is complete, present the results in a well-organized way:
	•	Forecast Plot: The primary output is a time series plot showing the historical Units Sold and the forecasted values into the future. This should be intuitive to read – include a legend indicating which line is actuals and which is forecast (and possibly which model produced which forecast if multiple are shown together). Use an interactive chart library (e.g., Plotly via st.plotly_chart) to allow zooming and hovering over points. For clarity, you might use different colors or line styles for each model’s forecast. Alternatively, use separate tabs or sections for each model: Streamlit’s st.tabs(["Prophet","ARIMA",...]) can create a tab for each model where you show that model’s forecast plot and metrics. This avoids cluttering one plot with many lines.
	•	Metrics: Show forecast accuracy metrics (RMSE, MAE) for each model in a readable format. A small table is a good choice, where rows are models and columns are RMSE and MAE. For example:
Model
RMSE
MAE
Prophet
X.yy
Y.yy
ARIMA
…
…
LSTM
…
…
XGBoost
…
…

Round the metric values to a reasonable number of decimal places for neatness. You can compute these metrics after generating forecasts for the validation portion of data. Display the table using st.table or st.dataframe or even st.write(pd.DataFrame(...)). If using tabs per model, you could show each model’s metrics next to its plot instead, but a combined comparison table is useful.
	•	Additional Insights: Optionally, include any other outputs that could be useful. For example, Prophet allows plotting trend and seasonal components (daily/weekly/yearly effects) – these could be shown under an expander or another tab for the Prophet model. For ARIMA, you might show the model summary (p,d,q) or the AIC value. For LSTM, perhaps the training loss curve (if training time is short enough to plot) or simply the information on how long training took. These are secondary, so prioritize the main forecast plot and metrics first.
	•	User Guidance & UX: Throughout the UI, add text instructions and labels to guide the user. For example, use st.markdown or st.write to add descriptions: “1. Upload your CSV file with weekly data. 2. Choose an item and forecast horizon. 3. Click ‘Run Forecast’ to see predictions.” This sequential guidance helps new users. Label charts and axes clearly (e.g., x-axis is date, y-axis is units sold). If a forecast looks suspicious (like negative or extremely large values), you might highlight that in the UI or handle it in code (e.g., clip negative predictions to zero with a warning). Keep the interface as uncluttered as possible – hide advanced options behind expanders or omit them in the MVP. A clean, focused UI will make the app more approachable.
Forecast Accuracy Metrics (RMSE, MAE)
	•	Calculating Metrics: After training the models, evaluate their performance using Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). These metrics will be computed on the validation/test set (the portion of historical data that was not used for training). For each model, take its predictions for each week of the test set and compare to the actual Units Sold. Compute:
	•	MAE: the average of the absolute differences between predicted and actual values. This tells us the typical magnitude of errors, treating all errors equally in linear scale. (For example, an MAE of 5 means on average the forecast is off by 5 units sold.)
	•	RMSE: the square root of the average of squared errors. This also gives an error in units of Units Sold, but penalizes larger errors more heavily due to squaring . RMSE is useful to highlight if a model occasionally makes big mistakes, as those will increase RMSE more than MAE. (For instance, if a model sometimes wildly misses a week’s sales, the RMSE will be much higher than the MAE.)
	•	Use a library like scikit-learn’s mean_absolute_error and mean_squared_error to get these values (taking the square root of MSE for RMSE). Ensure the metrics are computed only on weeks where actual values are present (the test set weeks). If the user doesn’t provide enough historical data to have a separate test set (say the dataset is very short), you can skip the metric calculation or perform a simple cross-validation (e.g., Prophet has a cross_val function) to still provide some sense of error. However, in most cases, a split of last N weeks for validation will be available.
	•	User-Friendly Display: Present the metrics clearly, as mentioned in the UI section. A single table comparing models is ideal so the user can easily see which model performed best on the historical test data. You might also highlight the best model for each metric (e.g., bold or color the lowest RMSE/MAE). It’s important to label the metrics in plain language, perhaps even adding a tooltip or footnote in the app explaining what RMSE and MAE mean in case the user is not familiar. For example, you can use st.metric components to show something like:
	•	Prophet – RMSE: 7.2, MAE: 5.8
	•	ARIMA – RMSE: 6.5, MAE: 5.0
and so on, but a tabular format is usually easier for side-by-side comparison.
	•	Interpretation: Provide a brief interpretation in the app of these metrics. For instance, “Lower values indicate more accurate forecasts . An RMSE of 10 means that the typical deviation of the forecast from actual sales is 10 units. Comparing models, ARIMA has the lowest RMSE in our example, suggesting it best captured the recent pattern.” This helps users understand the significance of the numbers. Emphasize that these metrics are calculated on historical data; actual future performance may differ. The app’s role is to give an idea of how well each model might predict by testing them on past known data.
	•	Updating Metrics: If the user changes the forecast horizon or selects a different item, recalcualte the metrics accordingly. For fairness in comparison, ensure all models are evaluated on the same time period. If needed, restrict the forecast horizon for evaluation (e.g., if the test set is 8 weeks, evaluate all models on 8-week forecasts). This way, the RMSE/MAE comparison is apples-to-apples across models.
Interactive Forecast Visualizations
	•	Time Series Plot: Use an interactive line chart to display the forecasts. A library like Plotly (via st.plotly_chart) can create an interactive time series plot where users can hover to see exact values and zoom into specific periods. Plot the historical Units Sold as a line or points (perhaps black dots or a solid line) up to the last available week. Then plot the forecasted values for each model:
	•	Prophet and ARIMA often yield one forecast series (with confidence intervals).
	•	LSTM and XGBoost yield one forecast series each.
Use different colors for each model’s forecast line (e.g., blue for Prophet, green for ARIMA, orange for XGBoost, red for LSTM). Include a legend identifying each. If confidence intervals are available (Prophet provides them; ARIMA can too), you can shade the region between the lower and upper bounds for those models (using Plotly’s fill between traces). This helps communicate uncertainty in the forecasts.
	•	Trend Visualization: Ensure the x-axis covers from the start of the historical period through the end of the forecast horizon. Mark the point where forecast begins (for example, a vertical dashed line separating historical vs forecast). This helps users see where the model is no longer using actual data. The title of the chart or an annotation can clarify “Historical vs Forecasted Units Sold”. Because the chart is interactive, the user can zoom into the forecast region or examine past trends.
	•	Model Comparison: Interactive elements allow users to toggle traces on and off. In Plotly, the legend can be clicked to hide/show a model’s forecast. This means the user can visually compare two models at a time or focus on one. By default, show all model forecasts. If the plot becomes too cluttered or if many models are selected, consider splitting the visualization:
	•	One combined plot for all forecasts (for high-level comparison).
	•	Individual plots per model (perhaps in separate tabs or stacked vertically) for detailed view including that model’s confidence intervals or components.
For example, under a “Prophet” tab, show Prophet’s forecast with its trend and seasonal component plots (Prophet’s plot_components output) to illustrate how the forecast was derived . Under “ARIMA” tab, maybe show the ACF/PACF of residuals or something diagnostic if desired. These additional plots enrich the app but are optional. Initially, focus on the primary forecast plot.
	•	Interactivity and UX: The interactive plot will let users mouse over points to see the exact predicted values per week per model. This is useful for reading the forecast numbers without digging into raw output tables. Ensure the hover info is clear (e.g., “Week of 2025-03-01: Prophet forecast = 120 units”). If possible, format dates nicely on the x-axis (Streamlit typically handles datetime indices well in plots). Also, consider using Streamlit’s built-in charts (like st.line_chart or Altair charts) if Plotly integration is too heavy – however, built-in charts might be less interactive. Plotly is a good choice for a richer experience (pan, zoom, toggle).
	•	Visual Aesthetics: Make the charts easy to read by setting axis labels (fig.update_layout(xaxis_title="Week", yaxis_title="Units Sold") in Plotly, for example). Use Streamlit’s light/dark theme appropriately (the chart will adapt to theme). Keep colorblind-friendly palette for lines if possible. Also, ensure that the chart is not too tall or too small – use use_container_width=True in st.plotly_chart to make it full width, and a reasonable height so that all data is visible without scrolling. If multiple plots are shown (say, one per model in tabs), each should have consistent axes ranges for easier comparison (you can fix the y-axis range to be the same across all or let them auto-scale individually – auto is fine for simplicity).
	•	Updating Plots: Whenever the user uploads new data, selects a different item, or runs a new forecast, make sure to update the plot accordingly. Clear out old plots or reuse the same plotting code so that stale results aren’t shown. Streamlit will rerun the script from top, so as long as the plotting code is conditional on having results, it will refresh. Consider using st.empty() containers that you fill with plots after computation, if you need more fine-grained control over update order.
Local Testing and Deployment
	•	Environment Setup: Develop and test the application locally on your machine. Ensure you have a Python environment (e.g., a virtualenv or conda environment) with all necessary packages installed. Key packages include: pandas (for data handling), streamlit (for the UI), forecasting libraries like fbprophet (a.k.a. prophet), statsmodels or pmdarima (for ARIMA), scikit-learn (for metrics and maybe data splitting), xgboost, and a deep learning framework (tensorflow or keras for LSTM). You might also need numpy (often installed with pandas) and possibly matplotlib/plotly for plotting. List these in a requirements.txt file for easy installation. For example, requirements.txt might contain:
streamlit==1.x  
prophet==1.x  
pmdarima==1.x  
scikit-learn==1.x  
xgboost==1.x  
tensorflow==2.x  (or keras==2.x)
plotly==5.x  
pandas==1.x  
numpy==1.x
This allows others (or you on another machine) to install the same versions. Install packages via pip (or conda as needed). Note: Prophet may require pystan or have compilation steps – using conda install -c conda-forge prophet can simplify that if pip gives issues .
	•	Running the App: Once the environment is ready and the code (e.g., app.py) is written, run streamlit run app.py from the terminal . This will start a local Streamlit server and open a browser window (usually at http://localhost:8501) to view the app. You can then interact with the interface, upload the sample CSV, and test the forecasting functionality end-to-end. During development, use Streamlit’s features to aid debugging: st.write() can print out data shapes or intermediate results, and errors will be shown in the terminal or browser with stack traces to help fix issues.
	•	Local Testing: Simulate various scenarios with test CSV data:
	•	A normal scenario with a reasonable amount of data (e.g., 2 years of weekly data for one item) to see if forecasts are produced and plots render.
	•	Edge cases: very small dataset (less than a few weeks of data – models may fail or produce trivial forecasts; handle this with a message like “Not enough data to forecast”), missing values in data, multiple items in the file, or extremely large values.
	•	Check performance: Does the app respond within a few seconds for each forecast? If the LSTM model is slow, you might test with fewer epochs or smaller network. Ensure the app doesn’t freeze; if it does, consider using st.cache_data to cache model results for a given input, so if you rerun with the same data it doesn’t retrain everything. Also, test memory usage if data or models are large.
	•	Verify that the metrics shown actually correspond to expected errors (you can manually calculate a small sample to confirm).
	•	Ensure that quitting and restarting the app (which resets state) works without issues.
	•	Iterative Development: Because you’re running locally, you can iterate quickly. After each code change, Streamlit can automatically reload the app if saved (or you can refresh the browser). This allows adding features one by one and verifying. For example, first get Prophet working (since it’s simpler), then add ARIMA, then XGBoost, then LSTM – testing each in isolation before enabling all together. Use version control (git) to track changes and revert if something breaks.
	•	Deployment (Optional): While not required for local use, note that once the app is stable, it could be deployed to a cloud platform or shared with others. Streamlit Community Cloud or Heroku could be used. But since the requirement is to run locally for testing, focus on that. To make it easy for someone else to run your app, provide clear instructions. For instance, in a README file, include steps: “Clone the repository, install requirements, then run streamlit run app.py” . If using Docker, you could also write a Dockerfile (the example Prophet app provided one ) so the entire app can run in a container, but that’s optional.
	•	Local Deployment Verification: Finally, simulate a “user acceptance test” on your local machine. Pretend you are the end user: follow the instructions fresh (maybe in a new environment) to ensure nothing is missing. This includes downloading a fresh CSV (or using the provided sample) and going through each feature. This helps catch any setup steps or hard-coded paths that only existed on your development setup. Once everything works smoothly – data loads correctly, forecasts are generated, metrics/plots display, and the UI is understandable – the development plan goals have been achieved. You now have a locally running, interactive forecast application that can be extended or deployed as needed.

